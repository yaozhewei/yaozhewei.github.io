---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---



About Me
======
Zhewei Yao is a principal researcher and R&D manager at Microsoft, working on efficient large scale training and inference. He obtained his Ph.D. degree from University of California at Berkeley, where he was a Ph.D. researcher in [BAIR](https://bair.berkeley.edu/), [RISELab](https://rise.cs.berkeley.edu/) ([former AMPLab](https://amplab.cs.berkeley.edu)), [BDD](https://deepdrive.berkeley.edu/), and [Math Department](https://math.berkeley.edu/). He was advised by [Michael Mahoney](https://www.stat.berkeley.edu/~mmahoney/), and he worked very closely with [Kurt Keutzer](https://people.eecs.berkeley.edu/~keutzer/). His research interest lies in computing statistics, optimization, and machine learning. Currently, he is interested in leveraging tools from randomized linear algebra to provide efficient and scalable solutions for large-scale optimization and learning problems. He is also working on the theory and application of deep learning. Before joining UC Berkeley, he received his B.S. in Math from [Zhiyuan Honor College](http://zhiyuan.sjtu.edu.cn/) at [Shanghai Jiao Tong University](http://en.sjtu.edu.cn/) (last update 8/23/2023).

Publications (See full list at [Google Scholar](https://scholar.google.com/citations?user=uG6vN6QAAAAJ&hl=en))
======
* <span style="color:blue">ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers</span>.\\
**Z. Yao**, R.Y. Aminabadi, M. Zhang, X. Wu, C. Li, Y. He\\
[arXiv](https://arxiv.org/pdf/2206.01861.pdf)\\
Accepted for publication, Proc. NeurIPS 2022
* <span style="color:blue">Extreme Compression for Pre-trained Transformers Made Simple and Efficient</span>.\\
X. Wu<sup>*</sup>, **Z. Yao<sup>*</sup>**, M. Zhang<sup>*</sup>, C. Li, Y. He\\
[arXiv](https://arxiv.org/pdf/2206.01859.pdf)\\
Accepted for publication, Proc. NeurIPS 2022
* <span style="color:blue">DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale</span>.\\
S. Rajbhandari, C. Li, **Z. Yao**, M. Zhang, R. Y. Aminabadi, A. A. Awan, J. Rasley, Y. He\\
[arXiv](https://arxiv.org/pdf/2201.05596.pdf)\\
Accepted for publication, Proc. ICML 2022

Selected Talks
======
* ICML'21 ([ICML](https://icml.cc/Conferences/2021/ScheduleMultitrack?event=10100))\\
Online (Jul, 2021)
* SIAM CSE'21: Beyond First Order Methods in Machine Learning Systems ([CSE](https://www.siam.org/conferences/cm/conference/cse21))\\
Online (Mar, 2021)
* AAAI'21 ([AAAI](https://aaai.org/Conferences/AAAI-21/))\\
Online (Feb, 2021)
* IEEE BigData'20 ([BigData](https://bigdataieee.org/BigData2020/))\\
Online (Dec, 2020), [slides](http://yaozhewei.github.io/files/pyhessian.pdf)
* Berkeley Real-time Intelligent Secure Explanaible Systems Lab Camp ([RiseLab](https://rise.cs.berkeley.edu/))\\
Online (Oct, 2020), [slides1](http://yaozhewei.github.io/files/adahessian.pdf) and [slides2](http://yaozhewei.github.io/files/pyhessian.pdf), [vedio](https://www.youtube.com/watch?v=sMDhXKqxfZc&list=PLTPaZLQlNIHo16Qq67CqWS6UKWrYREeKg&index=8)
* Fast.AI ([Fast.AI](https://www.fast.ai/))\\
Online (Oct, 2020), [slides](http://yaozhewei.github.io/files/adahessian.pdf), [vedio](https://www.youtube.com/watch?v=S87ancnZ0MM)
* Scalable Parallel Computing Lab ([SPCL](https://spcl.inf.ethz.ch/Bcast/))\\
Online (Oct, 2020), [slides](http://yaozhewei.github.io/files/adahessian.pdf), [vedio](https://youtu.be/AM9Bo8jLPpE)
* ICML'20 Workshop on Beyond First-Order Optimization Methods in Machine Learning ([Beyond](https://sites.google.com/view/optml-icml2020))\\
Online (July, 2020), [slides](http://yaozhewei.github.io/files/pyhessian.pdf), [vedio](https://icml.cc/virtual/2020/workshop/5737)
* Berkeley Real-time Intelligent Secure Explanaible Systems Lab Sponsor Retreat ([RiseLab](https://rise.cs.berkeley.edu/))\\
Tahoe Lake, CA, USA (May, 2020), [slides](http://yaozhewei.github.io/files/adahessian.pdf)
* NeurIPS'19 Workshop on Beyond First-Order Optimization Methods in Machine Learning ([Beyond](https://sites.google.com/site/optneurips19/))\\
Vancouver, Canada (December, 2019)
* DIMACS Workshop on Randomized Numerical Linear Algebra, Statistics, and Optimization ([DIMACS](http://dimacs.rutgers.edu/programs/sf/sf-optimization/))\\
Rutgers University, New Jersey, USA (September, 2019), [slides](http://yaozhewei.github.io/files/NLA.pdf)
* Computer Vision Panel ([IJCAI](https://www.ijcai19.org/)) \\
Macau, China (August, 2019), [slides](http://yaozhewei.github.io/files/ANODE.pdf)
* Randomized Algorithms for Optimization Problems in Statistics ([JSM](https://ww2.amstat.org/meetings/jsm/2019/onlineprogram/ActivityDetails.cfm?sessionid=217975))\\
Colorado Convention Center, Denver, Colorado, USA (July, 2019), [slides](http://yaozhewei.github.io/files/JSM.pdf)
* Berkeley Scientific Computing and Matrix Computations Seminar ([Link](https://math.berkeley.edu/~mgu/LAPACKSeminar.htm))\\
Berkeley, CA, USA (November, 2018), [slides](http://yaozhewei.github.io/files/absa.pdf)
* Berkeley Real-time Intelligent Secure Explanaible Systems Lab Sponsor Retreat ([RiseLab](https://rise.cs.berkeley.edu/))\\
Tahoe Lake, CA, USA (August, 2018), [slides](http://yaozhewei.github.io/files/absa.pdf)

Teaching
======
* [Stat 89A](https://www.stat.berkeley.edu/~mmahoney/s18-lads/), Spring 2018
* [Math 16A](https://math.berkeley.edu/~apaulin/16B_001%20(Spring%202017).html), Spring 2017
* Math 16A, Fall 2016
